---
title: "Homework 6"
author: Danyang Gui
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(p8105.datasets)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


### Problem 1


```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```


Start with one city.

```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```


Try this across cities.

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


### Problem 2


```{r}
bwt_df =
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
    malform = as.factor(malform)
  ) %>% 
  mutate(
    babysex = case_when(babysex == 1 ~ "male",
                        babysex == 2 ~ "female"),
    malform = case_when(malform == 0 ~ "absent",
                        malform == 1 ~ "present"),
    frace = case_when(frace == 1 ~ "White", 
                      frace == 2 ~ "Black", 
                      frace == 3 ~ "Asian", 
                      frace == 4 ~ "Puerto_Rican", 
                      frace == 8 ~ "Other"),
    mrace = case_when(mrace == 1 ~ "White", 
                      mrace == 2 ~ "Black", 
                      mrace == 3 ~ "Asian", 
                      mrace == 4 ~ "Puerto_Rican"))
```


### Model 1: 
I used stepwise regression. The 'stepAIC' function also helps to choose the best model.'stepAIC' uses AIC criterion and tries to find the model with the lowest AIC value. We ended up with a model predicted by 11 covariates including babysex, bhead, blength, delwt, fincome, gaweeks, mheight,  mrace, parity, ppwt, and smoken.
```{r}
library(MASS)
library(leaps)
full_model = lm(bwt ~., data = bwt_df)
stepwise_mod = stepAIC(full_model, direction = "backward", trace = FALSE)
summary(stepwise_mod)
```


```{r}
plot_model_1 = 
  bwt_df %>% 
  modelr::add_residuals(stepwise_mod) %>% 
  modelr::add_predictions(stepwise_mod) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  labs(
    title = "Residuals versus Fitted value plot for Model 1",
    x = "Predictor",
    y = "Residual"
  )
  
plot_model_1 
```



### Model 2: length at birth and gestational age as predictors

```{r}
main_only_mod = lm(bwt ~ blength + gaweeks, data = bwt_df)
summary(main_only_mod)
```

### Model 3: head circumference, length, sex, and all interactions (including the three-way interaction) 
```{r}
three_inter_mod = lm(bwt ~ bhead * blength * babysex, data = bwt_df)
summary(three_inter_mod)
```

### Cross-validation

```{r}
cv_df =
  crossv_mc(bwt_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    main_mod  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    three_mod = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    step_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                      gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x))) %>% 
  mutate(
    rmse_main = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_three = map2_dbl(three_mod, test, ~rmse(model = .x, data = .y)),
    rmse_step = map2_dbl(step_mod, test, ~rmse(model = .x, data = .y)))
```


```{r}
cv_df %>%
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() +
  labs(
    title = "RMSE of three models",
    x = "Model",
    y = "RMSE"
  )
```

The rmse plots shows that the model generated by stepwise method has the lowest RMSE.



### Problem 3

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

```{r}
set.seed(1)
boot_results =
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_id") %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results_tidy = map(models, broom::tidy),
    results_glance = map(models, broom::glance)
  ) %>% 
  unnest(results_tidy, results_glance) %>% 
  dplyr::select(strap_id, term, estimate, r.squared)
```

### Distribution of r squared

```{r}
estimate_rsquared = 
  boot_results %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of r squared")
  
estimate_rsquared
```

The distribution of r squared is relatively a normal distribution which is a little heavier left-tail. It is centered around 0.91 which suggests a high goodness-of-fit.


### Distribution of log(b0*b1)

```{r}
log_df = 
  boot_results %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(intercept = `(Intercept)`) %>% 
  mutate(
    log_b0_times_b1 = log(tmin * intercept)
  )
  
estimate_log = 
  log_df %>% 
  ggplot(aes(x = log_b0_times_b1)) +
  geom_density() +
  labs(
    title = "Distribution of log(b0*b1)")
  
estimate_log
```

The distribution of log(b0*b1) is relatively a normal distribution which is centered around 2.01.

### 95% Confidence Interval for r squared

```{r}
boot_results %>% 
  distinct(strap_id, r.squared) %>%
  summarise(lower = quantile(r.squared, 0.025),
            upper = quantile(r.squared, 0.975)) %>%
  knitr::kable(caption = "95% Confidence Interval for R squared")
```

The 95% Confidence Interval for R squared is (0.894, 0.927)

### 95% Confidence interval for log(b0*b1)

```{r}
log_df %>% 
  distinct(strap_id, log_b0_times_b1) %>%
  summarise(lower = quantile(log_b0_times_b1, 0.025),
            upper = quantile(log_b0_times_b1, 0.975)) %>%
  knitr::kable(caption = "95% Confidence Interval for log(b0*b1)")

```

The 95% Confidence Interval for log(b0*b1) is (1.965, 2.059)
